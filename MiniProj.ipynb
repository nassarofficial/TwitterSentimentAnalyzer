{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import preprocessor as p\n",
    "import enchant\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from __future__ import unicode_literals\n",
    "from stemming.porter2 import stem\n",
    "import ftfy\n",
    "import itertools\n",
    "from re import search\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tokenizers import twokenize\n",
    "from postaggers import arktagger\n",
    "from nltk import bigrams\n",
    "from nltk import trigrams\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from clusters import Clusters\n",
    "\n",
    "# Required:\n",
    "#pip install nltk\n",
    "#pip install stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter/Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "def lower_tokenize(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def wordcorrector(tweet):\n",
    "    d = enchant.Dict(\"en_US\")\n",
    "    for word in tweet:\n",
    "        if d.check(word) == False:\n",
    "            try:\n",
    "                word = d.suggest(word)[0]\n",
    "            except IndexError:\n",
    "                word = word\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return tweet\n",
    "\n",
    "def lowered(tweet):\n",
    "    for i in range(len(tweet)):\n",
    "        tweet[i] = tweet[i].encode(\"utf-8\").lower()\n",
    "    return tweet\n",
    "\n",
    "#stemming lemmitization\n",
    "stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "### Feature Extractor \\M/\n",
    "## Bag of Words\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "\n",
    "p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.HASHTAG, p.OPT.RESERVED, p.OPT.EMOJI, p.OPT.SMILEY, p.OPT.NUMBER)\n",
    "\n",
    "def filterer(input_data, pclean = 1, fixer = 1, whitespaces = 1, rm_numbers = 1, rm_punc = 1, corrector = 1, stemmer = 1, lemmatizer = 1, lowercase = 1):\n",
    "    filtered_tweets = []\n",
    "    for index, tweet in input_data[:40].iterrows():\n",
    "        ## options\n",
    "        if (pclean == 1):\n",
    "            tweet = p.clean(tweet[\"tweet\"])\n",
    "        if (fixer == 1):\n",
    "        ## fix words\n",
    "            tweet = ftfy.fix_text(tweet)\n",
    "        if (whitespaces == 1):\n",
    "        ## remove whitespaces\n",
    "            tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "        if (rm_numbers == 1):\n",
    "        ## remove numbers\n",
    "            tweet = ''.join([i for i in tweet if not i.isdigit()])\n",
    "        if (rm_punc == 1):\n",
    "        ## fix punctuation\n",
    "            tweet = \"\".join(c for c in tweet if c not in (\"'\",'\\\\','*',';','$','%','&','-','!','.',':','/','(',')','?','(',')',',','\"'))\n",
    "        ## tokenize\n",
    "        tweet = word_tokenize(tweet)\n",
    "        ## Word corrector\n",
    "        if (corrector == 1):\n",
    "            tweet = wordcorrector(tweet)\n",
    "        ## stemmer\n",
    "        if (stemmer == 1):\n",
    "            tweet = [stem(token) for token in tweet]\n",
    "        ## lemmatizer\n",
    "        if (lemmatizer == 1):\n",
    "            tweet = [lemmatiser.lemmatize(token, pos=\"v\") for token in tweet]\n",
    "        ## lower case\n",
    "        if (lowercase == 1):\n",
    "            tweet = lowered(tweet)\n",
    "        tweet = \" \".join(tweet).encode('utf-8')\n",
    "        filtered_tweets.append(tweet)\n",
    "    outputdata = filtered_tweets\n",
    "    return outputd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morphological Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "\n",
    "## Elongated Word\n",
    "def has_long(sentence):\n",
    "    elong = re.compile(\"([a-zA-Z])\\\\1{2,}\")\n",
    "    return bool(elong.search(sentence))\n",
    "\n",
    "def elongated(inputtweets):\n",
    "    elongated_words = []\n",
    "    for tweet in inputtweets:\n",
    "        if has_long(tweet) == True:\n",
    "            elongated_words.append(1)\n",
    "        else:\n",
    "            elongated_words.append(0)\n",
    "    return elongated_words\n",
    "\n",
    "def numberOfElongatedWords(sentence):\n",
    "    elong = re.compile(\"([a-zA-Z])\\\\1{2,}\")\n",
    "    return len([word for word in sentence.split() if elong.search(word)])\n",
    "\n",
    "## Existence of time\n",
    "def hastime(sentence):\n",
    "    d1= re.compile(r\"\\d{1,2}:\\d{1,2}.(AM|am|PM|pm|Pm|Am|a.m.|p.m.|A.M.|P.M.)+\")\n",
    "    d2= re.compile(r\"\\d{1,2}\\.\\d{1,2}.(AM|am|PM|pm|Pm|Am|a.m.|p.m.|A.M.|P.M.)+\")\n",
    "    d3= re.compile(r\"\\d{1,2}.(AM|am|PM|pm|Pm|Am|a.m.|p.m.|A.M.|P.M.)+\")\n",
    "    d4= re.compile(r\"at \\d{1,2} o'clock\")\n",
    "    if (bool(d1.search(sentence)) or bool(d2.search(sentence)) or bool(d3.search(sentence)) or bool(d4.search(sentence))):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "## Exitence of date\n",
    "def hasdate(sentence):\n",
    "    d1 = re.compile(r\"\\d{1,2}/\\d{1,2}/\\d{4}\")\n",
    "    d2 = re.compile(r\"\\d{1,2}-\\d{1,2}-\\d{4}\")\n",
    "    d3 = re.compile(r\"\\d{1,2}.\\d{1,2}.\\d{4}\")\n",
    "    d4 = re.compile(r\"\\d{1,2}/\\d{1,2}\")\n",
    "    d5 = re.compile(r\"\\d{1,2}-\\d{1,2}\")\n",
    "    d6 = re.compile(r\"\\d{1,2}.\\d{1,2}\")\n",
    "    d7 = re.compile(r\"\\d{1,2}(st|th|nd|rd)* of (Jan|jan|Feb|feb|Mar|mar|Apr|apr|May|may|June|june|July|july|Aug|aug|Sep|sep|Oct|oct|Nov|nov|Dec|dec)+\")\n",
    "    d8 = re.compile(r\"(Monday|monday|Tuesday|tuesday|Wednesday|wednesday|Thursday|thursday|Friday|friday|Saturday|saturday|Sunday|sunday)\")\n",
    "    d9 = re.compile(r\"(Jan|jan|Feb|feb|Mar|mar|Apr|apr|May|may|June|june|July|july|Aug|aug|Sep|sep|Oct|oct|Nov|nov|Dec|dec) \\d{1,2}(st|th|nd|rd)*\")\n",
    "    if (bool(d1.search(sentence)) or bool(d2.search(sentence)) or bool(d3.search(sentence)) or bool(d4.search(sentence)) or bool(d5.search(sentence)) or bool(d6.search(sentence)) or bool(d7.search(sentence)) or bool(d8.search(sentence)) or bool(d9.search(sentence))):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def countFullyCapitalizeTokens(tokens):\n",
    "    #return len([word for word in tokens if word==\"<allcaps>\"])\n",
    "    return len([word for word in tokens if word.isupper()])\n",
    "\n",
    "def countUpper(tokens):\n",
    "    return len([word for word in tokens if word[0].isupper()])\n",
    "\n",
    "def countexclama(message):\n",
    "    return message.count(\"!\")\n",
    "\n",
    "def countques(tokens):\n",
    "    x = 0\n",
    "    for token in tokens:\n",
    "        if token.count(\"?\") == len(token):\n",
    "            x+=1\n",
    "\n",
    "    return x\n",
    "\n",
    "def countdots(tokens):\n",
    "    return len([word for word in tokens if word==\"...\"])\n",
    "\n",
    "def hasslang(tokens,slangDictionary):\n",
    "    for token in tokens:\n",
    "        if token in slangDictionary:\n",
    "            return 1\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('twitter-2013train.txt', sep=\"\t\", header = None)\n",
    "data1 = pd.read_csv('twitter-2015train.txt', sep=\"\t\", header = None)\n",
    "data2 = pd.read_csv('twitter-2016train.txt', sep=\"\t\", header = None)\n",
    "\n",
    "frames = [data,data1,data2]\n",
    "result = pd.concat(frames,ignore_index=True)\n",
    "result.columns = [\"id\", \"polarity\", \"tweet\"]\n",
    "\n",
    "# result = result.drop_duplicates([\"tweet\"], keep='last')\n",
    "rescol = result[:40][\"tweet\"]\n",
    "\n",
    "filteredtweets = filterer(result, 1,1, 1,1,1,1,0,0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metamorphical Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "##### BEFORE FILTER\n",
    "## Elongated\n",
    "feat_elong = elongated(filteredtweets)\n",
    "feat_elong_num = [numberOfElongatedWords(tweet) for tweet in rescol]\n",
    "\n",
    "#Has time involved\n",
    "has_time = [hastime(tweet) for tweet in rescol]\n",
    "\n",
    "#has date\n",
    "has_date = [hasdate(tweet) for tweet in rescol]\n",
    "\n",
    "#capitalize num\n",
    "count_capital = [countFullyCapitalizeTokens(tknzr.tokenize(tweet)) for tweet in rescol]\n",
    "\n",
    "#capitalize upper first\n",
    "count_uppernum = [countUpper(tknzr.tokenize(tweet)) for tweet in rescol]\n",
    "\n",
    "#count ! marks\n",
    "count_excla = [countexclama(tweet) for tweet in rescol]\n",
    "\n",
    "#count ? marks\n",
    "count_ques = [countques(tknzr.tokenize(tweet)) for tweet in rescol]\n",
    "\n",
    "#count dots\n",
    "count_dots = [countdots(tknzr.tokenize(tweet)) for tweet in rescol]\n",
    "\n",
    "#has slang\n",
    "slangdata = [line.split(\"=\")[0] for line in open(\"lexicons/slangDict.txt\", 'r')]\n",
    "has_slang = [hasslang(tknzr.tokenize(tweet),slangdata) for tweet in rescol]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicon Feature Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLuster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Twitter Clusters\n",
    "def checkClusters(tokens,clusters):  \n",
    "    #initialize list with zeros\n",
    "    tags = [0] * len(clusters.keys)\n",
    "\n",
    "    c = []\n",
    "    for token in tokens:\n",
    "        c.append(clusters.d.get(token,\"no_cluster\"))\n",
    "\n",
    "    c = [x for x in c if x!=\"no_cluster\"] \n",
    "\n",
    "    for i in c:\n",
    "        tags[clusters.keys.index(i)] = 1\n",
    "    \n",
    "    return tags\n",
    "\n",
    "def loadClusters():\n",
    "    return Clusters.Clusters()\n",
    "\n",
    "clusters = loadClusters()\n",
    "cluster_tags = []\n",
    "for tweet in rescol:\n",
    "    tokenized = tknzr.tokenize(tweet)\n",
    "    cluster_tags.append(checkClusters(tokenized,clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Pos Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tags_list = []\n",
    "for tweet in rescol:\n",
    "    pos_tags_list.append(arktagger.pos_tag_list(tweet)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS grammatical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get Bigrams\n",
    "def get_bi(l):\n",
    "    b = []\n",
    "    for x in l:\n",
    "        b.append(list(bigrams(x)))\n",
    "\n",
    "    return b\n",
    "\n",
    "# Get Trigrams\n",
    "def get_tri(l):\n",
    "    tr = []\n",
    "    for x in l:\n",
    "        tr.append(list(trigrams(x)))\n",
    "\n",
    "    return tr\n",
    "\n",
    "def numberOfAdjectives(pos):\n",
    "    return len([x for x in pos if x==\"A\"])\n",
    "\n",
    "#calculate the number of adverbs\n",
    "def numberOfAdverbs(pos):\n",
    "    return len([x for x in pos if x==\"R\"])\n",
    "\n",
    "#calculate the number of interjections\n",
    "def numberOfIntejections(pos):\n",
    "    return len([x for x in pos if x==\"!\"])\n",
    "\n",
    "#calculate the number of verbs\n",
    "def numberOfVerbs(pos):\n",
    "    return len([x for x in pos if x==\"V\"])\n",
    "\n",
    "#calculate the number of nouns\n",
    "def numberOfNouns(pos):\n",
    "    return len([x for x in pos if x==\"N\"])\n",
    "\n",
    "#calculate the number of proper nouns\n",
    "def numberOfProperNouns(pos,tokens):\n",
    "    x = 0\n",
    "\n",
    "    for i in range(0,len(pos)):\n",
    "        try:\n",
    "            #pos tagger wrongly tags these words as a proper noun\n",
    "            if pos[i]==\"^\" and not(tokens[i]==\"<user>\" or tokens[i]==\"<sadface>\" or tokens[i]==\"<smile>\" or tokens[i]==\"<url>\"):\n",
    "                x+=1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return x\n",
    "            \n",
    "\n",
    "#calculate the number of urls\n",
    "def numberOfUrls(pos,tokens):\n",
    "    return (len([x for x in tokens if x==\"<url>\"]))\n",
    "\n",
    "#calculate the number of subjective emoticons\n",
    "def numberOfSubjectiveEmoticons(pos,tokens):\n",
    "    return (len([x for x in tokens if (x==\"<sadface>\" or x==\"<smile>\")]))\n",
    "\n",
    "#calculate the number of positive emoticons\n",
    "def numberOfPositiveEmoticons(tokens):\n",
    "   \n",
    "    return len([x for x in tokens if x==\"<smile>\"])\n",
    "\n",
    "#calculate the number of neutral emoticons\n",
    "def numberOfNeutralEmoticons(tokens):\n",
    "    \n",
    "    return len([x for x in tokens if x==\"<neutralface>\"])\n",
    "\n",
    "#calculate the number of negative emoticons\n",
    "def numberOfNegativeEmoticons(tokens):\n",
    "    return len([x for x in tokens if x==\"<sadface>\"])\n",
    "\n",
    "num_adj = [numberOfAdjectives(tweet) for tweet in pos_tags_list]\n",
    "num_adv = [numberOfAdverbs(tweet) for tweet in pos_tags_list]\n",
    "num_int = [numberOfIntejections(tweet) for tweet in pos_tags_list]\n",
    "num_ver = [numberOfVerbs(tweet) for tweet in pos_tags_list]\n",
    "num_nou = [numberOfNouns(tweet) for tweet in pos_tags_list]\n",
    "\n",
    "num_pno = []\n",
    "for i in range(len(pos_tags_list)):\n",
    "    num_pno.append(numberOfProperNouns(pos_tags_list[i],tknzr.tokenize(rescol[i])))\n",
    "\n",
    "num_url = []\n",
    "for i in range(len(pos_tags_list)):\n",
    "    num_url.append(numberOfUrls(pos_tags_list[i],tknzr.tokenize(rescol[i])))\n",
    "\n",
    "\n",
    "num_sem = []\n",
    "for i in range(len(pos_tags_list)):\n",
    "    num_sem.append(numberOfSubjectiveEmoticons(pos_tags_list[i],tknzr.tokenize(rescol[i])))\n",
    "\n",
    "num_pem = [numberOfPositiveEmoticons(tweet) for tweet in pos_tags_list]\n",
    "num_neu = [numberOfNeutralEmoticons(tweet) for tweet in pos_tags_list]\n",
    "num_neg = [numberOfNegativeEmoticons(tweet) for tweet in pos_tags_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#pos\n",
    "pos_feat = []\n",
    "\n",
    "for i in range(len(num_adj)):\n",
    "    featposrow=[]\n",
    "    sumof= num_adj[i] + num_adv[i] + num_int[i] + num_ver[i] + num_nou[i] + num_pno[i] + num_url[i] + num_sem[i] + num_pem[i] + num_neu[i] + num_neg[i]\n",
    "    featposrow.append(2*(num_adj[i]/float(sumof))-1)\n",
    "    featposrow.append(2*(num_adv[i]/float(sumof))-1)\n",
    "    featposrow.append(2*(num_int[i]/float(sumof))-1)\n",
    "    featposrow.append(2*(num_ver[i]/float(sumof))-1)\n",
    "    featposrow.append(2*(num_nou[i]/float(sumof))-1)\n",
    "    featposrow.append(2*(num_pno[i]/float(sumof))-1)\n",
    "    featposrow.append(2*(num_url[i]/float(sumof))-1)\n",
    "    featposrow.append(2*(num_sem[i]/float(sumof))-1)\n",
    "    featposrow.append(2*(num_pem[i]/float(sumof))-1)\n",
    "    featposrow.append(2*(num_neu[i]/float(sumof))-1)\n",
    "    featposrow.append(2*(num_neg[i]/float(sumof))-1)\n",
    "    pos_feat.append(featposrow)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Submission File Creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eng1 = [line.split(\",\")[0] for line in open(\"/home/nassar/Downloads/english_submission.csv\", 'r')]\n",
    "eng2 = [line.strip() for line in open(\"/home/nassar/aueb.twitter.sentiment/testres.csv\", 'r')]\n",
    "thelist = zip(eng1,[\"sentiment\"]+eng2)\n",
    "thefile = open('sub.csv', 'w')\n",
    "for i in range(len(thelist)):\n",
    "    thefile.write(\"%s\\n\" % ','.join([thelist[i][0],thelist[i][1]]))\n",
    "thefile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_tupled_list = [(x[0],x[1]) for x in zip(*thelist)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'id', u'218775148495515649'), (u'sentiment', 'negative')]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tupled_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
