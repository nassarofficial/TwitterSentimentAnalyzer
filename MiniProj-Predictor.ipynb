{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import preprocessor as p\n",
    "import enchant\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from __future__ import unicode_literals\n",
    "from stemming.porter2 import stem\n",
    "import ftfy\n",
    "import itertools\n",
    "from re import search\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tokenizers import twokenize\n",
    "from postaggers import arktagger\n",
    "from nltk import bigrams\n",
    "from nltk import trigrams\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from clusters import Clusters\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "from sklearn import svm\n",
    "\n",
    "# Required:\n",
    "#pip install nltk\n",
    "#pip install stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Filter/Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "def lower_tokenize(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def wordcorrector(tweet):\n",
    "    d = enchant.Dict(\"en_US\")\n",
    "    for word in tweet:\n",
    "        if d.check(word) == False:\n",
    "            try:\n",
    "                word = d.suggest(word)[0]\n",
    "            except IndexError:\n",
    "                word = word\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return tweet\n",
    "\n",
    "def lowered(tweet):\n",
    "    for i in range(len(tweet)):\n",
    "        tweet[i] = tweet[i].encode(\"utf-8\").lower()\n",
    "    return tweet\n",
    "\n",
    "#stemming lemmitization\n",
    "stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "### Feature Extractor \\M/\n",
    "## Bag of Words\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "\n",
    "p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.HASHTAG, p.OPT.RESERVED, p.OPT.EMOJI, p.OPT.SMILEY, p.OPT.NUMBER)\n",
    "\n",
    "def filterer(input_data, pclean = 1, fixer = 1, whitespaces = 1, rm_numbers = 1, rm_punc = 1, corrector = 1, stemmer = 1, lemmatizer = 1, lowercase = 1):\n",
    "    filtered_tweets = []\n",
    "#     for index, tweet in input_data[:40].iterrows():\n",
    "    for index, tweet in input_data.iterrows():\n",
    "        ## options\n",
    "        if (pclean == 1):\n",
    "            tweet = p.clean(tweet[\"tweet\"])\n",
    "        if (fixer == 1):\n",
    "        ## fix words\n",
    "            tweet = ftfy.fix_text(tweet)\n",
    "        if (whitespaces == 1):\n",
    "        ## remove whitespaces\n",
    "            tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "        if (rm_numbers == 1):\n",
    "        ## remove numbers\n",
    "            tweet = ''.join([i for i in tweet if not i.isdigit()])\n",
    "        if (rm_punc == 1):\n",
    "        ## fix punctuation\n",
    "            tweet = \"\".join(c for c in tweet if c not in (\"'\",'\\\\','*',';','$','%','&','-','!','.',':','/','(',')','?','(',')',',','\"'))\n",
    "        ## tokenize\n",
    "        tweet = word_tokenize(tweet)\n",
    "        ## Word corrector\n",
    "        if (corrector == 1):\n",
    "            tweet = wordcorrector(tweet)\n",
    "        ## stemmer\n",
    "        if (stemmer == 1):\n",
    "            tweet = [stem(token) for token in tweet]\n",
    "        ## lemmatizer\n",
    "        if (lemmatizer == 1):\n",
    "            tweet = [lemmatiser.lemmatize(token, pos=\"v\") for token in tweet]\n",
    "        ## lower case\n",
    "        if (lowercase == 1):\n",
    "            tweet = lowered(tweet)\n",
    "        tweet = \" \".join(tweet).encode('utf-8')\n",
    "        filtered_tweets.append(tweet)\n",
    "    outputdata = filtered_tweets\n",
    "    return outputdata\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Morphological Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "\n",
    "## Elongated Word\n",
    "def has_long(sentence):\n",
    "    elong = re.compile(\"([a-zA-Z])\\\\1{2,}\")\n",
    "    return bool(elong.search(sentence))\n",
    "\n",
    "def elongated(intweet):\n",
    "    state = 0\n",
    "    if has_long(intweet) == True:\n",
    "        state = 1\n",
    "    else:\n",
    "        state = 0\n",
    "    return state\n",
    "\n",
    "def numberOfElongatedWords(sentence):\n",
    "    elong = re.compile(\"([a-zA-Z])\\\\1{2,}\")\n",
    "    return len([word for word in sentence.split() if elong.search(word)])\n",
    "\n",
    "## Existence of time\n",
    "def hastime(sentence):\n",
    "    d1= re.compile(r\"\\d{1,2}:\\d{1,2}.(AM|am|PM|pm|Pm|Am|a.m.|p.m.|A.M.|P.M.)+\")\n",
    "    d2= re.compile(r\"\\d{1,2}\\.\\d{1,2}.(AM|am|PM|pm|Pm|Am|a.m.|p.m.|A.M.|P.M.)+\")\n",
    "    d3= re.compile(r\"\\d{1,2}.(AM|am|PM|pm|Pm|Am|a.m.|p.m.|A.M.|P.M.)+\")\n",
    "    d4= re.compile(r\"at \\d{1,2} o'clock\")\n",
    "    if (bool(d1.search(sentence)) or bool(d2.search(sentence)) or bool(d3.search(sentence)) or bool(d4.search(sentence))):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "## Exitence of date\n",
    "def hasdate(sentence):\n",
    "    d1 = re.compile(r\"\\d{1,2}/\\d{1,2}/\\d{4}\")\n",
    "    d2 = re.compile(r\"\\d{1,2}-\\d{1,2}-\\d{4}\")\n",
    "    d3 = re.compile(r\"\\d{1,2}.\\d{1,2}.\\d{4}\")\n",
    "    d4 = re.compile(r\"\\d{1,2}/\\d{1,2}\")\n",
    "    d5 = re.compile(r\"\\d{1,2}-\\d{1,2}\")\n",
    "    d6 = re.compile(r\"\\d{1,2}.\\d{1,2}\")\n",
    "    d7 = re.compile(r\"\\d{1,2}(st|th|nd|rd)* of (Jan|jan|Feb|feb|Mar|mar|Apr|apr|May|may|June|june|July|july|Aug|aug|Sep|sep|Oct|oct|Nov|nov|Dec|dec)+\")\n",
    "    d8 = re.compile(r\"(Monday|monday|Tuesday|tuesday|Wednesday|wednesday|Thursday|thursday|Friday|friday|Saturday|saturday|Sunday|sunday)\")\n",
    "    d9 = re.compile(r\"(Jan|jan|Feb|feb|Mar|mar|Apr|apr|May|may|June|june|July|july|Aug|aug|Sep|sep|Oct|oct|Nov|nov|Dec|dec) \\d{1,2}(st|th|nd|rd)*\")\n",
    "    if (bool(d1.search(sentence)) or bool(d2.search(sentence)) or bool(d3.search(sentence)) or bool(d4.search(sentence)) or bool(d5.search(sentence)) or bool(d6.search(sentence)) or bool(d7.search(sentence)) or bool(d8.search(sentence)) or bool(d9.search(sentence))):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def countFullyCapitalizeTokens(tokens):\n",
    "    #return len([word for word in tokens if word==\"<allcaps>\"])\n",
    "    return len([word for word in tokens if word.isupper()])\n",
    "\n",
    "def countUpper(tokens):\n",
    "    return len([word for word in tokens if word[0].isupper()])\n",
    "\n",
    "def countexclama(message):\n",
    "    return message.count(\"!\")\n",
    "\n",
    "def countques(tokens):\n",
    "    x = 0\n",
    "    for token in tokens:\n",
    "        if token.count(\"?\") == len(token):\n",
    "            x+=1\n",
    "\n",
    "    return x\n",
    "\n",
    "def countdots(tokens):\n",
    "    return len([word for word in tokens if word==\"...\"])\n",
    "\n",
    "def hasslang(tokens,slangDictionary):\n",
    "    for token in tokens:\n",
    "        if token in slangDictionary:\n",
    "            return 1\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "result = pd.read_csv('new_english_test.csv', sep=\",\", header = None)\n",
    "result.columns = [\"id\",\"tweet\"]\n",
    "filteredtweets = filterer(result, 1,1, 1,1,1,1,0,0,1)\n",
    "rescol = result[\"tweet\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Metamorphical Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "##### BEFORE FILTER\n",
    "## Elongated\n",
    "feat_elong = []\n",
    "feat_elong_num = []\n",
    "has_time = []\n",
    "has_date = []\n",
    "count_capital = []\n",
    "count_uppernum =[]\n",
    "count_excla = []\n",
    "count_ques =[]\n",
    "count_dots =[]\n",
    "slangdata = []\n",
    "has_slang = []\n",
    "\n",
    "slangdata = [line.split(\"=\")[0] for line in open(\"lexicons/slangDict.txt\", 'r')]\n",
    "\n",
    "for tweet in rescol:\n",
    "    #elongated words\n",
    "    feat_elong.append(elongated(tweet))\n",
    "    feat_elong_num.append(numberOfElongatedWords(tweet))\n",
    "    #Has time involved\n",
    "    has_time.append(hastime(tweet))\n",
    "    #has date\n",
    "    has_date.append(hasdate(tweet))\n",
    "    #capitalize num\n",
    "    count_capital.append(countFullyCapitalizeTokens(tknzr.tokenize(tweet)))\n",
    "    #capitalize upper first\n",
    "    count_uppernum.append(countUpper(tknzr.tokenize(tweet)))\n",
    "    #count ! marks\n",
    "    count_excla.append(countexclama(tweet))\n",
    "    #count ? marks\n",
    "    count_ques.append(countques(tknzr.tokenize(tweet)))\n",
    "    #count dots\n",
    "    count_dots.append(countdots(tknzr.tokenize(tweet)))\n",
    "    #has slang\n",
    "    has_slang.append(hasslang(tknzr.tokenize(tweet),slangdata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Lexicon Feature Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### CLuster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Twitter Clusters\n",
    "def checkClusters(tokens,clusters):  \n",
    "    #initialize list with zeros\n",
    "    tags = [0] * len(clusters.keys)\n",
    "\n",
    "    c = []\n",
    "    for token in tokens:\n",
    "        c.append(clusters.d.get(token,\"no_cluster\"))\n",
    "\n",
    "    c = [x for x in c if x!=\"no_cluster\"] \n",
    "\n",
    "    for i in c:\n",
    "        tags[clusters.keys.index(i)] = 1\n",
    "    \n",
    "    return tags\n",
    "\n",
    "def loadClusters():\n",
    "    return Clusters.Clusters()\n",
    "\n",
    "clusters = loadClusters()\n",
    "cluster_tags = []\n",
    "for tweet in rescol:\n",
    "    tokenized = tknzr.tokenize(tweet)\n",
    "    cluster_tags.append(checkClusters(tokenized,clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Get Pos Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pos_tags_list = []\n",
    "\n",
    "for tweet in filteredtweets:\n",
    "    pos_tags_list.append(arktagger.pos_tag_list(tweet)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### POS grammatical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get Bigrams\n",
    "def get_bi(l):\n",
    "    b = []\n",
    "    for x in l:\n",
    "        b.append(list(bigrams(x)))\n",
    "\n",
    "    return b\n",
    "\n",
    "# Get Trigrams\n",
    "def get_tri(l):\n",
    "    tr = []\n",
    "    for x in l:\n",
    "        tr.append(list(trigrams(x)))\n",
    "\n",
    "    return tr\n",
    "\n",
    "def numberOfAdjectives(pos):\n",
    "    return len([x for x in pos if x==\"A\"])\n",
    "\n",
    "#calculate the number of adverbs\n",
    "def numberOfAdverbs(pos):\n",
    "    return len([x for x in pos if x==\"R\"])\n",
    "\n",
    "#calculate the number of interjections\n",
    "def numberOfIntejections(pos):\n",
    "    return len([x for x in pos if x==\"!\"])\n",
    "\n",
    "#calculate the number of verbs\n",
    "def numberOfVerbs(pos):\n",
    "    return len([x for x in pos if x==\"V\"])\n",
    "\n",
    "#calculate the number of nouns\n",
    "def numberOfNouns(pos):\n",
    "    return len([x for x in pos if x==\"N\"])\n",
    "\n",
    "#calculate the number of proper nouns\n",
    "def numberOfProperNouns(pos,tokens):\n",
    "    x = 0\n",
    "\n",
    "    for i in range(0,len(pos)):\n",
    "        try:\n",
    "            #pos tagger wrongly tags these words as a proper noun\n",
    "            if pos[i]==\"^\" and not(tokens[i]==\"<user>\" or tokens[i]==\"<sadface>\" or tokens[i]==\"<smile>\" or tokens[i]==\"<url>\"):\n",
    "                x+=1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return x\n",
    "            \n",
    "\n",
    "#calculate the number of urls\n",
    "def numberOfUrls(pos,tokens):\n",
    "    return (len([x for x in tokens if x==\"<url>\"]))\n",
    "\n",
    "#calculate the number of subjective emoticons\n",
    "def numberOfSubjectiveEmoticons(pos,tokens):\n",
    "    return (len([x for x in tokens if (x==\"<sadface>\" or x==\"<smile>\")]))\n",
    "\n",
    "#calculate the number of positive emoticons\n",
    "def numberOfPositiveEmoticons(tokens):\n",
    "   \n",
    "    return len([x for x in tokens if x==\"<smile>\"])\n",
    "\n",
    "#calculate the number of neutral emoticons\n",
    "def numberOfNeutralEmoticons(tokens):\n",
    "    \n",
    "    return len([x for x in tokens if x==\"<neutralface>\"])\n",
    "\n",
    "#calculate the number of negative emoticons\n",
    "def numberOfNegativeEmoticons(tokens):\n",
    "    return len([x for x in tokens if x==\"<sadface>\"])\n",
    "\n",
    "num_adj = []\n",
    "num_adv = []\n",
    "num_int = []\n",
    "num_ver = []\n",
    "num_nou = []\n",
    "num_pno = []\n",
    "num_url = []\n",
    "num_sem = []\n",
    "num_pem = []\n",
    "num_neu = []\n",
    "num_neg = []\n",
    "\n",
    "print len(pos_tags_list)\n",
    "lengthx = len(rescol)\n",
    "\n",
    "\n",
    "# for i in rescol.index:\n",
    "for i in range(len(filteredtweets)):\n",
    "    num_adj.append(numberOfAdjectives(pos_tags_list[i]))\n",
    "    num_adv.append(numberOfAdverbs(pos_tags_list[i]))\n",
    "    num_int.append(numberOfIntejections(pos_tags_list[i]))\n",
    "    num_ver.append(numberOfVerbs(pos_tags_list[i]))\n",
    "    num_nou.append(numberOfNouns(pos_tags_list[i]))\n",
    "    num_pem.append(numberOfPositiveEmoticons(pos_tags_list[i]))\n",
    "    num_neu.append(numberOfNeutralEmoticons(pos_tags_list[i]))\n",
    "    num_neg.append(numberOfNegativeEmoticons(pos_tags_list[i]))\n",
    "    num_pno.append(numberOfProperNouns(pos_tags_list[i],tknzr.tokenize(filteredtweets[i])))\n",
    "    num_url.append(numberOfUrls(pos_tags_list[i],tknzr.tokenize(filteredtweets[i])))\n",
    "    num_sem.append(numberOfSubjectiveEmoticons(pos_tags_list[i],tknzr.tokenize(filteredtweets[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print len(filteredtweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Negation Lexicon\n",
    "def negationlex():\n",
    "    path = \"lexicons/negations.txt\"\n",
    "    negfile = open(path,\"r\")\n",
    "    negList = []\n",
    "    for line in negfile.readlines():\n",
    "        line = line.decode('utf8')\n",
    "        negList.append(line[0:len(line)-1])\n",
    "    negfile.close()\n",
    "    return negList\n",
    "\n",
    "def isneg(tokens,neg_list):\n",
    "    for token in tokens:\n",
    "        if token in neg_list:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "## NRC\n",
    "nrc_sl = \"lexicons/NRC/NRC-Hashtag-Sentiment-Lexicon-v0.1/unigrams-pmilexicon.txt\"\n",
    "nrc_md = \"lexicons/NRC/MaxDiff-Twitter-Lexicon/Maxdiff-Twitter-Lexicon_-1to1.txt\"\n",
    "nrc_140 = \"lexicons/NRC/Sentiment140AffLexNegLex/S140-AFFLEX-NEGLEX-unigrams.txt\"\n",
    "\n",
    "def nrc_lexicons(path):\n",
    "    d_unigrams = {}\n",
    "    f = open(path)\n",
    "    for line in f.readlines():\n",
    "        line = line.decode('utf8')\n",
    "        try:\n",
    "            key = line.split(\"\\t\")[0]\n",
    "            value = line.split(\"\\t\")[1]\n",
    "            d_unigrams[key]=float(value)\n",
    "        except IndexError, e:\n",
    "            print path\n",
    "            print \"liner\", line\n",
    "            continue\n",
    "    f.close()\n",
    "    return d_unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Feature Calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#pos\n",
    "f_num_adj = []\n",
    "f_num_adv = []\n",
    "f_num_int = []\n",
    "f_num_ver = []\n",
    "f_num_nou = []\n",
    "f_num_pno = []\n",
    "f_num_url = []\n",
    "f_num_sem = []\n",
    "f_num_pem = []\n",
    "f_num_neu = []\n",
    "f_num_neg = []\n",
    "\n",
    "## calculate pos\n",
    "for i in range(len(num_adj)):\n",
    "    try:\n",
    "        sumof= num_adj[i] + num_adv[i] + num_int[i] + num_ver[i] + num_nou[i] + num_pno[i] + num_url[i] + num_sem[i] + num_pem[i] + num_neu[i] + num_neg[i]\n",
    "        f_num_adj.append(2*(num_adj[i]/float(sumof))-1)\n",
    "        f_num_adv.append(2*(num_adv[i]/float(sumof))-1)\n",
    "        f_num_int.append(2*(num_int[i]/float(sumof))-1)\n",
    "        f_num_ver.append(2*(num_ver[i]/float(sumof))-1)\n",
    "        f_num_nou.append(2*(num_nou[i]/float(sumof))-1)\n",
    "        f_num_pno.append(2*(num_pno[i]/float(sumof))-1)\n",
    "        f_num_url.append(2*(num_url[i]/float(sumof))-1)\n",
    "        f_num_sem.append(2*(num_sem[i]/float(sumof))-1)\n",
    "        f_num_pem.append(2*(num_pem[i]/float(sumof))-1)\n",
    "        f_num_neu.append(2*(num_neu[i]/float(sumof))-1)\n",
    "        f_num_neg.append(2*(num_neg[i]/float(sumof))-1)\n",
    "    except ZeroDivisionError:\n",
    "        print i\n",
    "        f_num_adj.append(0)\n",
    "        f_num_adv.append(0)\n",
    "        f_num_int.append(0)\n",
    "        f_num_ver.append(0)\n",
    "        f_num_nou.append(0)\n",
    "        f_num_pno.append(0)\n",
    "        f_num_url.append(0)\n",
    "        f_num_sem.append(0)\n",
    "        f_num_pem.append(0)\n",
    "        f_num_neu.append(0)\n",
    "        f_num_neg.append(0)\n",
    "\n",
    "\n",
    "## calculate negation\n",
    "neglist = negationlex()\n",
    "\n",
    "neglexfeat = []\n",
    "for tweet in filteredtweets:\n",
    "    neglexfeat.append(isneg(tknzr.tokenize(tweet),neglist))\n",
    "\n",
    "## calculate NRC Lexicons\n",
    "\n",
    "dict_nrc_sl = nrc_lexicons(nrc_sl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Zip features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "features = []\n",
    "features = np.asarray(zip(feat_elong_num, has_time, has_date, count_capital, count_uppernum,\n",
    "                          count_excla, count_ques, count_dots, has_slang,f_num_adj,f_num_adv,\n",
    "                          f_num_int,f_num_ver,f_num_nou,f_num_pno,f_num_url,f_num_sem,f_num_pem,\n",
    "                          f_num_neu,f_num_neg))\n",
    "\n",
    "## Just adding cluster tags to the list of features\n",
    "cluster_tags = np.asarray(cluster_tags)\n",
    "features = np.concatenate((features, cluster_tags),axis=-1)\n",
    "# features = np.column_stack((result[\"id\"].values,result[\"polarity\"].values,result[\"tweet\"].values,features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('model_nolexicon.pkl', 'rb') as handle:\n",
    "    model = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('model_nolexicon.pkl', 'wb') as output:\n",
    "    pickle.dump(model, output, pickle.HIGHEST_PROTOCOL)\n",
    "print \"Model saved...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Submission File Creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "eng1 = [line.split(\",\")[0] for line in open(\"/home/nassar/Downloads/english_submission.csv\", 'r')]\n",
    "eng2 = [line.strip() for line in open(\"/home/nassar/aueb.twitter.sentiment/testres.csv\", 'r')]\n",
    "thelist = zip(eng1,[\"sentiment\"]+eng2)\n",
    "thefile = open('sub.csv', 'w')\n",
    "for i in range(len(thelist)):\n",
    "    thefile.write(\"%s\\n\" % ','.join([thelist[i][0],thelist[i][1]]))\n",
    "thefile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_tupled_list = [(x[0],x[1]) for x in zip(*thelist)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_tupled_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
